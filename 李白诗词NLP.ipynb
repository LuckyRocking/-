{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import csv\n",
    "import jieba\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras import Sequential\n",
    "import keras_bert \n",
    "from keras import models\n",
    "from keras import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import LSTM,Embedding,Dense,GRU,SimpleRNN\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取李白诗词数据集\n",
    "libai=pd.read_csv(r'C:\\Users\\Administrator\\Desktop\\李白诗词.txt',encoding='utf-8')\n",
    "#得到李白诗词数据列表\n",
    "poetry_lis=[''.join(i.split()) for i in libai['      李白诗词      ']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poetry_optimize(poetry_lis:list):\n",
    "    '''并去除<p>卷、「诗名进行去除。将每行诗句以逗号、句号分隔开，'''\n",
    "    new_poetrycontnt=[]\n",
    "    for every_poetry in poetry_lis:\n",
    "        poetry_content=every_poetry.split('<')[0]\n",
    "        if '「'  not in poetry_content:\n",
    "            new_poetrycontnt.append(''.join(re.split('[，。]',poetry_content)))\n",
    "    return new_poetrycontnt\n",
    "\n",
    "#jieba分词\n",
    "def get_worddict(new_poetrycontnt:list):\n",
    "    '''将列表中的每一个诗句进行分词，使用jieba分词得到全部词，形成词典\n",
    "    '''\n",
    "    Token_poetry_content=[]\n",
    "    word_dict=dict()\n",
    "    for sentence in new_poetrycontnt:\n",
    "       for word in jieba.lcut(sentence,cut_all=True):\n",
    "            if word not in word_dict:\n",
    "                word_dict[word]=1\n",
    "            else:\n",
    "                word_dict[word]+=1\n",
    "            Token_poetry_content.append(word)\n",
    "    return word_dict,Token_poetry_content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.980 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "#统计词频\n",
    "#查看词频图\n",
    "new_poetrycontnt=poetry_optimize(poetry_lis)\n",
    "word_dict,Token_poetry_content=get_worddict(new_poetrycontnt)\n",
    "#pd.Series(word_dict).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将词映射称数字\n",
    "def word_to_index(word_dict):\n",
    "   '''将字映射成字符'''\n",
    "   word_to_index=dict()\n",
    "   for index,word in enumerate(word_dict):\n",
    "      word_to_index[word]=index\n",
    "   return word_to_index\n",
    "word_index=word_to_index(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#数据处理\n",
    "#########将李白诗词进行分词，并拿key进行替代\n",
    "def poetry_to_sequence(Token_poetry_content,word_index):\n",
    "    sequence=[]\n",
    "    for word in Token_poetry_content:\n",
    "        sequence.append(word_index[word])\n",
    "    return sequence\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence=poetry_to_sequence(Token_poetry_content,word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "################滑动窗口\n",
    "def slide_window(sequence,windowsize=5):\n",
    "    '''滑动窗口采集数据'''\n",
    "    feature_data=[]\n",
    "    label_data=[]\n",
    "    for _ in range(len(sequence)-windowsize):\n",
    "        feature,label=[[i] for i in sequence[_:_+windowsize]],sequence[_+windowsize]\n",
    "\n",
    "        feature_data.append(feature)\n",
    "        label_data.append(label)\n",
    "    return feature_data,label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data,label_data=slide_window(sequence)\n",
    "label_data=to_categorical(label_data)\n",
    "model=models.Sequential()\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.LSTM(32,input_shape=(len(feature_data),1),activation='relu',return_sequences=True))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.LSTM(16,activation='elu'))\n",
    "model.add(layers.Dense(10129,activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics='acc')\n",
    "model.fit(np.array(feature_data),label_data,epochs=5,batch_size=64,validation_data=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findmax_index(nums):\n",
    "    index=0\n",
    "    max=0\n",
    "    for index,number in enumerate(nums):\n",
    "        if number>max:\n",
    "            max=number\n",
    "            index=index\n",
    "    return max,index\n",
    "def findmax_index(nums):\n",
    "    maxindex=0\n",
    "    max=0\n",
    "    for index,number in enumerate(nums):\n",
    "        if number>max:\n",
    "            max=number\n",
    "            maxindex=index\n",
    "    return max,maxindex\n",
    "\n",
    "def get_predict(model,input_word,word_index,maxlen=100):\n",
    "    for num in range(maxlen): \n",
    "        token_input_word=jieba.lcut(input_word)\n",
    "        token_input_word_to_index=[[word_index[word]] if word in word_index else [np.random.randint(20000)] for word in token_input_word ]\n",
    "        if len(token_input_word)<5:\n",
    "            for _ in range(0,5-len(token_input_word_to_index)):\n",
    "                token_input_word_to_index.append([np.random.randint(20000)])\n",
    "        predict_lis=model.predict(token_input_word_to_index[-5:])\n",
    "        maxproba,maxprebawordindex=findmax_index(predict_lis[0])\n",
    "        index_to_word=dict([(j,i) for i,j in word_index.items()])\n",
    "        next_word=index_to_word[maxprebawordindex]\n",
    "        input_word+=next_word\n",
    "    print(input_word+next_word)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "input='啊啊啊啊啊'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [159], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m get_predict(model,\u001b[39minput\u001b[39;49m,word_index)\n",
      "Cell \u001b[1;32mIn [135], line 2\u001b[0m, in \u001b[0;36mget_predict\u001b[1;34m(input_word, word_index, maxlen)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_predict\u001b[39m(input_word,word_index,maxlen\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     now_nums_word\u001b[39m=\u001b[39mjieba\u001b[39m.\u001b[39;49mlcut(input_word)\n\u001b[0;32m      3\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(maxlen\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(now_nums_word)):\n\u001b[0;32m      4\u001b[0m         new_word_index\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m10129\u001b[39m)\n",
      "File \u001b[1;32me:\\Python\\lib\\site-packages\\jieba\\__init__.py:357\u001b[0m, in \u001b[0;36mTokenizer.lcut\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlcut\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 357\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcut(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n",
      "File \u001b[1;32me:\\Python\\lib\\site-packages\\jieba\\__init__.py:300\u001b[0m, in \u001b[0;36mTokenizer.cut\u001b[1;34m(self, sentence, cut_all, HMM, use_paddle)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    291\u001b[0m \u001b[39mThe main function that segments an entire sentence that contains\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[39mChinese characters into separated words.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[39m    - HMM: Whether to use the Hidden Markov Model.\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    299\u001b[0m is_paddle_installed \u001b[39m=\u001b[39m check_paddle_install[\u001b[39m'\u001b[39m\u001b[39mis_paddle_installed\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m--> 300\u001b[0m sentence \u001b[39m=\u001b[39m strdecode(sentence)\n\u001b[0;32m    301\u001b[0m \u001b[39mif\u001b[39;00m use_paddle \u001b[39mand\u001b[39;00m is_paddle_installed:\n\u001b[0;32m    302\u001b[0m     \u001b[39m# if sentence is null, it will raise core exception in paddle.\u001b[39;00m\n\u001b[0;32m    303\u001b[0m     \u001b[39mif\u001b[39;00m sentence \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(sentence) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32me:\\Python\\lib\\site-packages\\jieba\\_compat.py:79\u001b[0m, in \u001b[0;36mstrdecode\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(sentence, text_type):\n\u001b[0;32m     78\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 79\u001b[0m         sentence \u001b[39m=\u001b[39m sentence\u001b[39m.\u001b[39;49mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     80\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mUnicodeDecodeError\u001b[39;00m:\n\u001b[0;32m     81\u001b[0m         sentence \u001b[39m=\u001b[39m sentence\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mgbk\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "get_predict(model,input,word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57507, 10129)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_randompredict(input_word,word_index,maxlen=20):\n",
    "    now_nums_word=jieba.lcut(input_word)\n",
    "    for _ in range(maxlen-len(now_nums_word)):\n",
    "        new_word_index=np.random.randint(10129)\n",
    "        index_to_word=dict([(j,i) for i,j in word_index.items()])\n",
    "        new_word=index_to_word[new_word_index]\n",
    "        input_word+=new_word\n",
    "    \n",
    "    return input_word\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'啊啊啊啊啊蕖阐苦难蹳逵百万明晨久留心乱窗户沅切秦汉珠玉川北时光炼'"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_word=get_randompredict(input,word_index,maxlen=20)\n",
    "input_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(input_word):\n",
    "    new_str=[]\n",
    "    for i in range(len(input_word)):\n",
    "      if i//5==0:\n",
    "        new_str.append(input_word[i:i+5])\n",
    "    return ','.join(new_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputsentence=optimize(str(get_randompredict(input,word_index,maxlen=100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'啊啊啊啊啊,啊啊啊啊著,啊啊啊著书,啊啊著书鸿,啊著书鸿沟'"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputsentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'啊啊啊啊啊两客后门枉杀老僧济悸优游昨夜仙郎坚开白葭一经倾国改化作沙伊川崆峒携标猿啼炼气茫户长豕翮残袖长南京涂泥中期河湾不啻交相喷薄残兵苔绿色一往玩忽一枝荆扉夷门下去四争战比颇然后巾东妓女徒劳夕烟洗染古意莫邪重价云山永隐然霹雳逸兴峰嶂右军丰年怡然三边凋落嗽饲寥寥下列苑向海准回向襜脱倾倒鸣凤硾云汉千里名花玉露生遍宝刀写意垂青得免贫病陆相娱长江罗江奔走军威'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(get_randompredict(input,word_index,maxlen=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e450050b432e843bda3c41bf3272c133bfc370a7003f3e377e27f87a49ce1127"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
